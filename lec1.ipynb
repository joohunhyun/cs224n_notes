{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1 : intro and Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The big question of NLP is : **How can we represent the meaning of a word?**\n",
    "\n",
    "### 1.1 Wordnet\n",
    "- To answer the aforementioned question, the previously utilized NLP solution was **WordNet**\n",
    "- In traditional NLP, words are represented as discrete symbols (as one-hot-vectors)\n",
    "- Problem\n",
    "  - If a user searches for “Seattle motel”, we would like to matchdocuments containing “Seattle hotel”.\n",
    "  - However, the two vectors below are orthogonal, so there is no similarity between the two in one-hot-vectors\n",
    "    - Usually, cosine similarity is used to find similarity between words\n",
    "  - Basically, it indexes words to integers (no similarity can be found)\n",
    "\n",
    "```\n",
    "motel = [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
    "hotel = [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
    "```\n",
    "- Solution\n",
    "  - Could try to rely on WordNet’s list of synonyms to get similarity?\n",
    "    - But it is well-known to fail badly: incompleteness, etc.\n",
    "  - Instead: learn to encode similarity in the vectors themselves\n",
    "\n",
    "### 1.2 Wordvec : Representing words by context\n",
    "\n",
    "- **Distributional Semantics** : A word's meaning is given by the words that frequently appear close by\n",
    "- This idea allowed the breakthrough in NLP\n",
    "- AKA word Vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. WordNet\n",
    "- **Wordnet** is a lexical database of semantic relations between words in English first created by CogSys Lab of Princeton University.\n",
    "- It includes N, V, ADJ, ADV but omits PREP, DET, and other function words.\n",
    "- WordVec for other langauges exists too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 WordNet Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading nltk and wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synsets for the word \"invite\" in WordNet:\n",
      "\n",
      " [Synset('invite.n.01'), Synset('invite.v.01'), Synset('invite.v.02'), Synset('tempt.v.03'), Synset('invite.v.04'), Synset('invite.v.05'), Synset('invite.v.06'), Synset('invite.v.07'), Synset('receive.v.05')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "print('Synsets for the word \"invite\" in WordNet:\\n\\n', wn.synsets('invite'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Way one--------------------\n",
      "Synsets for the noun \"invite\" in WordNet:\n",
      "\n",
      " [Synset('invite.n.01')]\n",
      "\n",
      "\n",
      "--------------------Way two--------------------\n",
      "Synsets for the noun \"invite\" in WordNet:\n",
      "\n",
      " [Synset('invite.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# We can constrain the search by specifying the part of speech\n",
    "# parts of speech available: ADJ, ADV, ADJ_SAT, NOUN, VERB\n",
    "# ADJ_SAT: see https://stackoverflow.com/questions/18817396/what-part-of-speech-does-s-stand-for-in-wordnet-synsets\n",
    "\n",
    "# Way one\n",
    "print(f'{\"-\"*20}Way one{\"-\"*20}')\n",
    "print('Synsets for the noun \"invite\" in WordNet:\\n\\n', wn.synsets('invite', pos=wn.NOUN))\n",
    "\n",
    "# Way two\n",
    "print(f'\\n\\n{\"-\"*20}Way two{\"-\"*20}')\n",
    "# pos: {'n':'noun', 'v':'verb', 's':'adj (s)', 'a':'adj', 'r':'adv'}\n",
    "print('Synsets for the noun \"invite\" in WordNet:\\n\\n', [s for s in wn.synsets('invite') if s.pos()=='n'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Definition--------------------\n",
      "The definition for invite as a noun:\n",
      "\n",
      " a colloquial expression for invitation\n",
      "\n",
      "\n",
      "--------------------Examples--------------------\n",
      "The definition for invite as a noun:\n",
      "\n",
      " [\"he didn't get no invite to the party\"]\n",
      "\n",
      "\n",
      "--------------------Hypernyms--------------------\n",
      "The hypernyms for invite as a noun:\n",
      "\n",
      " [Synset('invitation.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# check definition of a synset\n",
    "print(f'{\"-\"*20}Definition{\"-\"*20}')\n",
    "print('The definition for invite as a noun:\\n\\n', wn.synset('invite.n.01').definition())\n",
    "\n",
    "# check the related examples\n",
    "print(f'\\n\\n{\"-\"*20}Examples{\"-\"*20}')\n",
    "print('The definition for invite as a noun:\\n\\n', wn.synset('invite.n.01').examples())\n",
    "\n",
    "# check the hypernyms\n",
    "print(f'\\n\\n{\"-\"*20}Hypernyms{\"-\"*20}')\n",
    "print('The hypernyms for invite as a noun:\\n\\n', wn.synset('invite.n.01').hypernyms())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Limitations of Wordnet\n",
    "- Requires human labor\n",
    "  - Impossible to update every word\n",
    "  - Expensive\n",
    "- Missing **nuance**\n",
    "  - \"proficient\" is listed as a synoynm for \"good\"\n",
    "- Misses new words\n",
    "  - badass, nifty, etc\n",
    "- Cannot compute word similarity accurately (score range : 0~1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path similarity between cat(noun) and dog(noun):  0.2\n"
     ]
    }
   ],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "print('The path similarity between cat(noun) and dog(noun): ', dog.path_similarity(cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word Vectors(Embeddings)\n",
    "\n",
    "To address the limitations of wordnets(OHV), word vectors were introduced.\n",
    "\n",
    "\n",
    "### 3.1 word vectors\n",
    "- When a word *w* appears in a text, the **context** is the set of words that appear nearby.\n",
    "- **Context words** build up a representation of *w*\n",
    "- A dense **vector** for each word is created, measuring similarity as the vector dot product\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/j/l1/j1.png\" alt=\"Word Vectors\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "**Word Space**\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/j/l1/j2.png\" alt=\"word vector\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "- Note that:\n",
    "  - has, have, had are grouped together\n",
    "  - come, go are closely grouped\n",
    "  - Usually 500~1000 dimensions per word\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Word2vec Algorithm Basics\n",
    "\n",
    "**Word2Vec(Mikolov et al. 2013)** is a framework for learning word vectors\n",
    "\n",
    "\n",
    "### 3.3 Basic idea\n",
    "\n",
    "How it works:\n",
    "\n",
    "1. Get a large **corpus**(latin word for body) of text\n",
    "2. Create a vector for each word in a fixed vocabulary\n",
    "3. Go through each position *t* in the text, which has center word *c* and context words *o*\n",
    "4. Find the probability of *o* given *c*(or vice versa) using the similarity of word vectors for *c* and *o*\n",
    "5. Keep adjusting this to maximize the probability\n",
    "\n",
    "**core idea** : What is the probability of a word occuring in the context of the  center word?\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/j/l1/j3.png\" alt=\"calc\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "If the window = 2, then it predicts the likelihood of the 2 words that come before and after the center word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Math behind Word2Vec\n",
    "\n",
    "\n",
    "#### 3.4.1 Objective/Loss/Cost Function\n",
    "\n",
    "\n",
    "Likelihood is the measure of how \"fit\" a given data sample is to a model. For each position $t = 1, ..., T $, predict context words within a window of fixed size $ m $, given center word $w_j$.\n",
    "\n",
    "$$\n",
    "Likelihood = L(\\theta) = \\prod_{t=1}^{T} \\prod_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}} P(w_{t+j} \\mid w_t; \\theta)\n",
    "$$\n",
    "\n",
    "$ \\theta $  in $ L(\\theta) $ is all variables to be optimized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective Function(AKA : loss function)** : this is what is to be minimized. Low loss means greater accuracy. formula:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{T} \\log L(\\theta) = -\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}} \\log P(w_{t+j} \\mid w_t; \\theta)\n",
    "$$\n",
    "\n",
    "- This is the average negative log likelihood\n",
    "- Why log is required?\n",
    "  -  because multiplying many proababilities will make the number close to 0\n",
    "  -  logging it prevents this phenomenon\n",
    "\n",
    "Question : How is $ P $ calculated? -> **prediction function**\n",
    "\n",
    "Answer : we will use two vectors for word $ w $\n",
    "  - $ v_w $ when $ w $ is a center word\n",
    "  - $ u_w $ when $ w $ is a context word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Log-Likelihood: 257.3551120942153\n"
     ]
    }
   ],
   "source": [
    "# Negative Log-Likelihood Function\n",
    "def negative_log_likelihood(X, theta):\n",
    "    \"\"\"\n",
    "    Computes the Negative Log-Likelihood (NLL) for a Gaussian distribution.\n",
    "    \"\"\"\n",
    "    return -np.sum(-0.5 * (X - theta)**2 - 0.5 * np.log(2 * np.pi))\n",
    "\n",
    "nll_val = negative_log_likelihood(X, theta_test)\n",
    "print(\"Negative Log-Likelihood:\", nll_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 Prediction function\n",
    "\n",
    "**Core idea** : Predicting words that appear left and right of a given context word\n",
    "\n",
    "For a given center word $c$ the probability of a context word $o$ appearing is:\n",
    "\n",
    "$$\n",
    "P(o \\mid c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w \\in V} \\exp(u_w^T v_c)}\n",
    "$$\n",
    "\n",
    "**Numerator** \n",
    "\n",
    "$$ \\exp(u_o^T v_c) $$\n",
    "\n",
    "- Calculates the similarity between target and context word\n",
    "- $ u_o^T v_c $ is the dot product between vector representations of $o$ and $c$ -> measure of how similar the two words are in embedding space\n",
    "  - larger dot product = larger probability\n",
    "  - dot product is a real number, so exponentiation is taken\n",
    "    - exponentiation makes any number positive\n",
    "- Applying the exponential function (exp) ensures that the result is always positive and helps in normalizing the values\n",
    "\n",
    "\n",
    "**Denominator**\n",
    "\n",
    "$$ \\sum_{w \\in V} \\exp(u_w^T v_c) $$\n",
    "\n",
    "- Calculates sum over all possible words (over all vocabulary)\n",
    "- Ensures that the probability values sum to 1, making the formula a valid probability distribution\n",
    "\n",
    "\n",
    "#### 3.4.3 Prediction function = softmax function\n",
    "\n",
    "- TLDR: The softmax function converts a vector of real numbers into a probability distribution\n",
    "- Why use this?\n",
    "  - The output values lie in the range of (0,1) and sum to 1, making them interpretable as probabilities.\n",
    "  - In the **final layer** of a NN, softmax ensures that predictions are probabilities over multiple classes.\n",
    "- \"max\" because amplifies probability of largest $ x_i $\n",
    "- \"soft\" because still assigns some probability to smaller $ x_i $\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{\\exp(x_i)}{\\sum_{j=1}^{n} \\exp(x_j)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Probabilities: [0.07444069 0.13176847 0.10419385 0.06380236 0.10424873 0.11576871\n",
      " 0.11774626 0.06498345 0.09594868 0.12709881]\n"
     ]
    }
   ],
   "source": [
    "# Prediction Function\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Computes the softmax function for an array x.\n",
    "    \"\"\"\n",
    "    exp_x = np.exp(x - np.max(x))  # For numerical stability\n",
    "    return exp_x / exp_x.sum()\n",
    "\n",
    "def predict_probabilities(word_vector, context_vector, vocabulary):\n",
    "    \"\"\"\n",
    "    Computes P(o|c) using softmax, given word embeddings.\n",
    "    \"\"\"\n",
    "    scores = np.dot(vocabulary, context_vector)  # Dot product with all words\n",
    "    return softmax(scores)\n",
    "\n",
    "\n",
    "# Prediction Example\n",
    "vocab_size = 10\n",
    "embedding_dim = 5\n",
    "vocabulary = np.random.rand(vocab_size, embedding_dim)  # Fake word embeddings\n",
    "context_vector = np.random.rand(embedding_dim)\n",
    "probabilities = predict_probabilities(vocabulary, context_vector, vocabulary)\n",
    "print(\"Prediction Probabilities:\", probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent\n",
    "\n",
    "To train a model, the parameters are adjusted gradually to minimize the loss function discussed above. **Gradient descent** is used to minimized the loss by - *walking down* the gradient - to find the local minima(where it is 0) of the function.\n",
    "\n",
    "### 4.1 Types of Gradient Descent \n",
    "\n",
    "**Batch Gradient descent** is *computationally expensive*. **Stochastic Gradient Descent** is often used instead (often used as DL optimizer). SGD samples expectations to make it stochastically updated.Newer DL optimizers such as Adam is also functions based on SGD.\n",
    "\n",
    "### 4.2 Math\n",
    "\n",
    "#### 4.2.1 Prerequisites\n",
    "\n",
    "#### 4.2.2 Gradient Descent for Loss Function\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
