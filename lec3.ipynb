{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3 : Backpropagation and NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER : Named Entity Recognition\n",
    "\n",
    "- TLDR: a common NLP task. Labeling words that are entities (places, names, dates, etc)\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/j/l3/1.png\" alt=\"Word Vectors\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "### How NER works\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/j/l3/2.png\" alt=\"Word Vectors\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "steps\n",
    "1. Form a **context window** $ X_{window} $ by finding the wordvectors of each word within the window. The resulting vector $ X_{window } $ is a 5 dimensional vector $R^{5d}$\n",
    "2. Put they through a NN layer\n",
    "3. Feed it through logistic classifiers for each entity type. At the end, it will have the probabillty of a word being each entity type (probability of the word being a location, person, etc ...)\n",
    "\n",
    "#### Why $f$ (non-linearity) is needed?\n",
    "\n",
    "- NN do function approximation tasks such as regression or classification.\n",
    "- Without non-linearlity, DNN cannot do antythin more than a linear transform.\n",
    "- By adding more layers that include non-linearities, they can approximate complex functions such as the ones below. (as $M$ grows bigger, the more accurately represented the function)\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/j/l3/3.png\" alt=\"Word Vectors\" width=\"200\"/>\n",
    "</p>\n",
    "\n",
    "#### Types of non-linearities\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/j/l3/4.png\" alt=\"Word Vectors\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "- Professor's note : for DNN, **ReLU** is the first thing to try out. It trains quickly and performs well due to good gradient backflow.\n",
    "- **GELU** is frequently used with transformers(BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent  Calculation\n",
    "\n",
    "Update equation :\n",
    "\n",
    "$ \\theta^{new} = \\theta^{old} - \\alpha \\nabla_{\\theta}J(\\theta)$\n",
    "\n",
    "How can we compute gradient? -> $\\nabla_{\\theta}J(\\theta)$\n",
    "\n",
    "1. by hand\n",
    "2. algorithmically using backpropagation\n",
    "\n",
    "\n",
    "### Computing Gradient : Jacobian way"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
