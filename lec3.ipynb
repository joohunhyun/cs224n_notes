{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3 : Backpropagation and NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER : Named Entity Recognition\n",
    "\n",
    "- TLDR: a common NLP task. Labeling words that are entities (places, names, dates, etc)\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/j/l3/1.png\" alt=\"Word Vectors\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "### How NER works\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/j/l3/2.png\" alt=\"Word Vectors\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "steps\n",
    "1. Form a **context window** $ X_{window} $ by finding the wordvectors of each word within the window. The resulting vector $ X_{window } $ is a 5 dimensional vector $R^{5d}$\n",
    "2. Put they through a NN layer\n",
    "3. Feed it through logistic classifiers for each entity type. At the end, it will have the probabillty of a word being each entity type (probability of the word being a location, person, etc ...)\n",
    "\n",
    "#### Why $f$ (non-linearity) is needed?\n",
    "\n",
    "- NN do function approximation tasks such as regression or classification.\n",
    "- Without non-linearlity, DNN cannot do antythin more than a linear transform.\n",
    "- By adding more layers that include non-linearities, they can approximate complex functions such as the ones below. (as $M$ grows bigger, the more accurately represented the function)\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/j/l3/3.png\" alt=\"Word Vectors\" width=\"200\"/>\n",
    "</p>\n",
    "\n",
    "#### Types of non-linearities\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/j/l3/4.png\" alt=\"Word Vectors\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "- Professor(Byung Jun Lee)'s note : for DNN, **ReLU** is the first thing to try out. It trains quickly and performs well due to good gradient backflow.\n",
    "- **GELU** is frequently used with transformers(BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent  Calculation\n",
    "\n",
    "Update equation :\n",
    "\n",
    "$ \\theta^{new} = \\theta^{old} - \\alpha \\nabla_{\\theta}J(\\theta)$\n",
    "\n",
    "How can we compute gradient? -> $\\nabla_{\\theta}J(\\theta)$\n",
    "\n",
    "1. by hand\n",
    "2. algorithmically using backpropagation\n",
    "\n",
    "\n",
    "### Computing Gradients by hand : the Jacobian way\n",
    "\n",
    "- Given a function with $m$ outputs and $n$ inputs\n",
    "\n",
    "$$ f(x) = [f_1(x_1,x_2, ..., x_n), ... , f_m(x_1,x_2,...,x_n)] $$\n",
    "\n",
    "- It's Jacobian is an $ m * n $ matrix of partial derivatives.\n",
    "- It shows every possible output variable in respect to input variables.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x}  =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Jacobian formulaes\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial x} (Wx + b) = W$$\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial b} (Wx + b) = I = \\text{Identity matrix} $$\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial x} (u^T h) = h^T$$\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
