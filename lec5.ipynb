{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5 : Language Models and RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques to train Large NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "- Regularization is used for models with many parameters (large NN) -> makes function smoother\n",
    "- e.g. L2 regularization on loss function\n",
    "\n",
    "\n",
    "\n",
    "- Classical View : Regularization used to prevent **overfitting**\n",
    "  - The function is made smoother so overfitting is mitigated\n",
    "- Now: Regularization produces models that generalize well for \"big\" models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "- Also considered regularization, but is a more stochastic one.\n",
    "- Randomly set inputs or hidden units to 0. By doing this, the NN becomes more robust and no longer depends on specific inputs (it needs to learn how to predict output).\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/j/l5/1.png\" alt=\"Word Vectors\" width=\"500\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Initialization\n",
    "\n",
    "- Optimizers are things that make large NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "- For complex NN, it uses a more sophisticated adptive optimizers\n",
    "- Optimizers\n",
    "\n",
    "Adagrad -> Simplest member of family, but tends to “stall early”\n",
    "• RMSprop\n",
    "• Adam -> A fairly good, safe place to begin in many cases\n",
    "• AdamW\n",
    "• NAdamW ->Can be better with word vectors (W) and for speed (Nesterov acceleration)\n",
    "\n",
    "\n",
    "- Rule of thumb is to go with Adam(safe to begin in many cases, used in many NNs today)\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modeling\n",
    "\n",
    "**Language Modeling** is the task of predicting what word comes next.\n",
    "\n",
    "Formal Definition : Language models, given a sequence of words $ x^1, x^2,...., x^t$, computes the probability distribution of the next word $x^{t+1}$\n",
    "\n",
    "Language model : system that assigns probability to a piece of text\n",
    "\n",
    "If we have some text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance of Language Modeling \n",
    "\n",
    "- Language Modeling is a benchmark task that helps us measure our progress on\n",
    "predicting language use\n",
    "- Language Modeling is a subcomponent of many NLP tasks, especially those involving generating text or estimating the probability of text:\n",
    "  - Predictive typing\n",
    "  - Speech recognition\n",
    "  - Handwriting recognition\n",
    "  - Spelling/grammar correction\n",
    "  - Authorship identification\n",
    "  - Machine translation\n",
    "  - Summarization\n",
    "  - Dialogue\n",
    "  - etc\n",
    "- ChatGPT is also a LM\n",
    "  - LLMs have become so powerful, they can do all above tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next-word prediction\n",
    "\n",
    "```A Strong LM like LLMs can do many things```\n",
    "\n",
    "- GPT-2\n",
    "- GPT-3\n",
    "  - The model can perform tasks when given a few examples (‘in-context learning’)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram LM\n",
    "\n",
    "- Pre-Deep Learning era method of learning a Language Model\n",
    "- **n-gram** is a chunk of n-consecutive words\n",
    "  - unigram : \"the\"\n",
    "  - bigrams : \"the students\"\n",
    "- TL;DR : collect statistics about how frequent different n-grams are in a given corpora, and use these to predict next word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparsity Problem of n-grams\n",
    "\n",
    "**Sparsity Problem** occurs when you try to increase the number of $n$ in n-grams.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/j/l5/2.png\" alt=\"Word Vectors\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "- Increasing $n$ makes the sparsity problem exponentially greater\n",
    "- Typically we can't have n bigger than 5.\n",
    "\n",
    "N-gram Language Models\n",
    "- You need to consider **more than three words** at a time to model language well. Or else, it will be **incoherent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fixed window Neural LM\n",
    "\n",
    "\n",
    "Problems :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN\n",
    "\n",
    "- Solves the problems of fixed window Neural LM\n",
    "\n",
    "How it works\n",
    "\n",
    "\n",
    "Advantages\n",
    "\n",
    "\n",
    "Disadvantages\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
